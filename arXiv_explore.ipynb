{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cs: from_2016-01-01_to_2016-12-31\n",
      "     math: from_2016-01-01_to_2016-07-01\n",
      "     math: from_2016-07-02_to_2016-12-31\n",
      "  physics: from_2016-01-01_to_2016-05-01\n",
      "  physics: from_2016-05-02_to_2016-09-01\n",
      "  physics: from_2016-09-02_to_2016-12-31\n",
      "    q-bio: from_2016-01-01_to_2016-12-31\n",
      "    q-fin: from_2016-01-01_to_2016-12-31\n",
      "     stat: from_2016-01-01_to_2016-12-31\n"
     ]
    }
   ],
   "source": [
    "# load the csv's obtained with 'arXiv_metadata_harvester.ipynb'\n",
    "\n",
    "#dir_ = \"arXivMeta_completed/\"\n",
    "dir_ = \"arXivMeta_sample/\"\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# create dict of file-lists dfs = {'cs': [file1, ...], 'math': ..., etc.}\n",
    "for file in os.listdir(dir_):\n",
    "    info = re.search(r'arXivMeta_(.+?)_(.+?)\\.(\\w+?)', file)\n",
    "    cat = info.group(1)\n",
    "    date = info.group(2)\n",
    "    print(f'{cat:>9}: {date}')\n",
    "    \n",
    "    file_path = os.path.join(dir_, file)\n",
    "    \n",
    "    if cat not in dfs:\n",
    "        dfs[cat] = [file_path]\n",
    "    else:\n",
    "        dfs[cat].append(file_path)\n",
    "\n",
    "# create dict of data_frames dfs = {'cs': pd.DataFrame(), 'math': ..., etc.}       \n",
    "def getdf(file_list):\n",
    "    cat_dfs = [pd.read_csv(file, delimiter='\\t') for file in file_list]\n",
    "    return pd.concat(cat_dfs)\n",
    "\n",
    "dfs = {cat: getdf(cat_files) for (cat, cat_files) in dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and select the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prim_cat</th>\n",
       "      <th>sec_cats</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0189</td>\n",
       "      <td>math</td>\n",
       "      <td>math.GR</td>\n",
       "      <td>Monoid generalizations of the Richard Thompson...</td>\n",
       "      <td>The groups G_{k,1} of Richard Thompson and G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.1313</td>\n",
       "      <td>math</td>\n",
       "      <td>math.GT math.CO</td>\n",
       "      <td>Mutant knots and intersection graphs</td>\n",
       "      <td>We prove that if a finite order knot invaria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.2146</td>\n",
       "      <td>math</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>Connected Edge-Disjoint Unions of Tur\\'an Graphs</td>\n",
       "      <td>A finite connected graph $G_r^\\sigma$ is con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id prim_cat         sec_cats  \\\n",
       "0  0704.0189     math          math.GR   \n",
       "1  0704.1313     math  math.GT math.CO   \n",
       "2  0704.2146     math          math.CO   \n",
       "\n",
       "                                               title  \\\n",
       "0  Monoid generalizations of the Richard Thompson...   \n",
       "1               Mutant knots and intersection graphs   \n",
       "2   Connected Edge-Disjoint Unions of Tur\\'an Graphs   \n",
       "\n",
       "                                            abstract  \n",
       "0    The groups G_{k,1} of Richard Thompson and G...  \n",
       "1    We prove that if a finite order knot invaria...  \n",
       "2    A finite connected graph $G_r^\\sigma$ is con...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have for example\n",
    "\n",
    "dfs['math'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cs', True),\n",
       " ('math', True),\n",
       " ('physics', True),\n",
       " ('q-bio', True),\n",
       " ('q-fin', True),\n",
       " ('stat', True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there are no duplicates (at least by 'id')\n",
    "\n",
    "[(cat, 0 == len(df.id) - len(df.id.unique())) for (cat,df) in dfs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all data_frames into one\n",
    "\n",
    "full_df = pd.concat(dfs.values())\n",
    "\n",
    "# get a 10% sample to play with\n",
    "\n",
    "sample_df = full_df.sample(frac=0.1, random_state=123)\n",
    "sample_df.reset_index(inplace=True)\n",
    "len(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cats(df):\n",
    "    \n",
    "    cats = df.prim_cat.unique()\n",
    "    record_counts = []\n",
    "    \n",
    "    for cat in cats:\n",
    "        count = sum(df.prim_cat == cat)\n",
    "        record_counts.append( [cat, count/1000] )\n",
    "    \n",
    "    record_counts = sorted(record_counts, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    record_counts = np.array(record_counts)\n",
    "    record_counts_df = pd.DataFrame({'categories': record_counts[:,0], '1000 papers': record_counts[:,1]}).set_index('categories')\n",
    "    display(record_counts_df)\n",
    "    record_counts_df.astype('float').plot.bar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000 papers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>physics</th>\n",
       "      <td>8.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>4.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat</th>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-bio</th>\n",
       "      <td>0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q-fin</th>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1000 papers\n",
       "categories            \n",
       "physics          8.003\n",
       "math             4.546\n",
       "cs                2.62\n",
       "stat             0.565\n",
       "q-bio            0.295\n",
       "q-fin            0.099"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2843a64b898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the number of papers in each class\n",
    "# the dataset is highly unbalanced, there's orders of magnitude between the most and the least frequent class\n",
    "\n",
    "show_cats(sample_df)\n",
    "\n",
    "# the world of quantative finance seems to be less keen on spending time writing papers than physicists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get primary_categories (prim_cat) and text (title+abstract) only\n",
    "# and safe those in 'bare data_frames'\n",
    "\n",
    "def strip(df):\n",
    "    df_2 = pd.concat([(df.title + \" \" + df.abstract).astype('str'),\n",
    "                      df.prim_cat.astype('category')], axis=1)\n",
    "    df_2.columns = ['text', 'label']\n",
    "    return df_2\n",
    "\n",
    "bare_dfs = {cat: strip(df) for (cat, df) in dfs.items()}\n",
    "\n",
    "bare_sample_df = strip(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The IBM 2016 Speaker Recognition System   In this paper we describe the recent advancements made in the IB...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 2(2S+1)- Formalism and Its Connection with Other Descriptions   In the framework of the Joos-Weinberg ...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regression-based Intra-prediction for Image and Video Coding   By utilizing previously known areas in an i...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            text  \\\n",
       "0  The IBM 2016 Speaker Recognition System   In this paper we describe the recent advancements made in the IB...   \n",
       "1  The 2(2S+1)- Formalism and Its Connection with Other Descriptions   In the framework of the Joos-Weinberg ...   \n",
       "2  Regression-based Intra-prediction for Image and Video Coding   By utilizing previously known areas in an i...   \n",
       "\n",
       "  label  \n",
       "0    cs  \n",
       "1  math  \n",
       "2    cs  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we have for example\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 110):\n",
    "    display(bare_sample_df[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One could also have a look at the papers' secondary categories of which there are almost 200 and there can be many such categories per paper. Maybe later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO side-project:\n",
    "# classify by subcategories (first entries in the list in 'sec_cats' of each paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          \n",
      "0704.0213  0      cs.CC\n",
      "0704.0802  0      cs.IT\n",
      "           1    math.IT\n",
      "0704.1409  0      cs.LG\n",
      "           1      cs.AI\n",
      "0704.2452  0      cs.IT\n",
      "           1    math.IT\n",
      "0706.0534  0    stat.ML\n",
      "           1      cs.IT\n",
      "           2    math.IT\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc-phys</th>\n",
       "      <th>adap-org</th>\n",
       "      <th>alg-geom</th>\n",
       "      <th>ao-sci</th>\n",
       "      <th>astro-ph</th>\n",
       "      <th>astro-ph.CO</th>\n",
       "      <th>astro-ph.EP</th>\n",
       "      <th>astro-ph.GA</th>\n",
       "      <th>astro-ph.HE</th>\n",
       "      <th>astro-ph.IM</th>\n",
       "      <th>...</th>\n",
       "      <th>q-fin.TR</th>\n",
       "      <th>quant-ph</th>\n",
       "      <th>solv-int</th>\n",
       "      <th>stat.AP</th>\n",
       "      <th>stat.CO</th>\n",
       "      <th>stat.ME</th>\n",
       "      <th>stat.ML</th>\n",
       "      <th>stat.OT</th>\n",
       "      <th>stat.TH</th>\n",
       "      <th>supr-con</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0704.0005</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0006</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0020</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           acc-phys  adap-org  alg-geom  ao-sci  astro-ph  astro-ph.CO  \\\n",
       "id                                                                       \n",
       "0704.0005         0         0         0       0         0            0   \n",
       "0704.0006         0         0         0       0         0            0   \n",
       "0704.0020         0         0         0       0         0            0   \n",
       "\n",
       "           astro-ph.EP  astro-ph.GA  astro-ph.HE  astro-ph.IM    ...     \\\n",
       "id                                                               ...      \n",
       "0704.0005            0            0            0            0    ...      \n",
       "0704.0006            0            0            0            0    ...      \n",
       "0704.0020            0            0            0            0    ...      \n",
       "\n",
       "           q-fin.TR  quant-ph  solv-int  stat.AP  stat.CO  stat.ME  stat.ML  \\\n",
       "id                                                                            \n",
       "0704.0005         0         0         0        0        0        0        0   \n",
       "0704.0006         0         0         0        0        0        0        0   \n",
       "0704.0020         0         0         0        0        0        0        0   \n",
       "\n",
       "           stat.OT  stat.TH  supr-con  \n",
       "id                                     \n",
       "0704.0005        0        0         0  \n",
       "0704.0006        0        0         0  \n",
       "0704.0020        0        0         0  \n",
       "\n",
       "[3 rows x 173 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot-encode the \n",
    "ids_with_cats_ugly = full_df[['id', 'sec_cats']].set_index('id').sec_cats.str.split(expand=True).stack()\n",
    "print(ids_with_cats_ugly[:10])\n",
    "\n",
    "id_with_cats_df = pd.get_dummies(ids_with_cats_ugly).groupby(level=0).sum()\n",
    "id_with_cats_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_with_cats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO take care of adap-org (nlin.AO), chao-dyn(nlin.CD), patt-sol (nlin.PS) etc.:\n",
    "\n",
    "# Each category should be in the format 'general.specic' (e.g. 'astr-ph.CO') or just 'general' (e.g. 'astro-ph')\n",
    "# but some of our columns are assigned to 'specific' in the spelled-out format (eg. chao-dyn instead of nlin.CD)\n",
    "# See https://arxiv.org/ and http://arxitics.com/help/categories\n",
    "\n",
    "# for column in id_with_cats_df:\n",
    "#     print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstracts of scientific papers tend to be written in a formal style, not contain typos, no direct citations, little references, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities and Regions in Britain through hierarchical percolation   Urban systems present hierarchical structures at many different scales. These\r\n",
      "are observed as administrative regional delimitations which are the outcome of\r\n",
      "complex geographical, political and historical processes which leave almost\r\n",
      "indelible footprints on infrastructure such as the street network. In this work\r\n",
      "we uncover a set of hierarchies in Britain at different scales using\r\n",
      "percolation theory on the street network and on its intersections which are the\r\n",
      "primary points of interaction and urban agglomeration. At the larger scales,\r\n",
      "the observed hierarchical structures can be interpreted as regional fractures\r\n",
      "of Britain, observed in various forms, from natural boundaries, such as\r\n",
      "National Parks, to regional divisions based on social class and wealth such as\r\n",
      "the well-known North-South divide. At smaller scales, cities are generated\r\n",
      "through recursive percolations on each of the emerging regional clusters. We\r\n",
      "examine the evolution of the morphology of the system as a whole, by measuring\r\n",
      "the fractal dimension of the clusters at each distance threshold in the\r\n",
      "percolation. We observe that this reaches a maximum plateau at a specific\r\n",
      "distance. The clusters defined at this distance threshold are in excellent\r\n",
      "correspondence with the boundaries of cities recovered from satellite images,\r\n",
      "and from previous methods using population density.\r\n",
      "\n",
      "---\n",
      "Identification of repeats in DNA sequences using nucleotide distribution\r\n",
      "  uniformity   Repetitive elements are important in genomic structures, functions and\r\n",
      "regulations, yet effective methods in precisely identifying repetitive elements\r\n",
      "in DNA sequences are not fully accessible, and the relationship between\r\n",
      "repetitive elements and periodicities of genomes is not clearly understood. We\r\n",
      "present an $\\textit{ab initio}$ method to quantitatively detect repetitive\r\n",
      "elements and infer the consensus repeat pattern in repetitive elements. The\r\n",
      "method uses the measure of the distribution uniformity of nucleotides at\r\n",
      "periodic positions in DNA sequences or genomes. It can identify periodicities,\r\n",
      "consensus repeat patterns, copy numbers and perfect levels of repetitive\r\n",
      "elements. The results of using the method on different DNA sequences and\r\n",
      "genomes demonstrate efficacy and accuracy in identifying repeat patterns and\r\n",
      "periodicities. The complexity of the method is linear with respect to the\r\n",
      "lengths of the analyzed sequences.\r\n",
      "\n",
      "---\n",
      "EPR Pairs, Local Projections and Quantum Teleportation in Holography   In this paper we analyze three quantum operations in two dimensional\r\n",
      "conformal field theories (CFTs): local projection measurements, creations of\r\n",
      "partial entanglement between two CFTs, and swapping of subsystems between two\r\n",
      "CFTs. We also give their holographic duals and study time evolutions of\r\n",
      "entanglement entropy. By combining these operations, we present an analogue of\r\n",
      "quantum teleportation between two CFTs and give its holographic realization. We\r\n",
      "introduce a new quantity to probe tripartite entanglement by using local\r\n",
      "projection measurement.\r\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# take a look at a few abstracts\n",
    "\n",
    "import random\n",
    "  \n",
    "for _ in range(3):\n",
    "    print(bare_sample_df.text.iloc[random.choice(range(len(bare_sample_df)))])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One messy but informative kind of writing they have are LateX formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Functional determinants, index theorems, and exact quantum black hole\\r\\n  entropy   The exact quantum entropy of BPS black holes can be evaluated using\\r\\nlocalization in supergravity. An important ingredient in this program, that has\\r\\nbeen lacking so far, is the one-loop effect arising from the quadratic\\r\\nfluctuations of the exact deformation (the $Q\\\\mathcal{V}$ operator). We compute\\r\\nthe fluctuation determinant for vector multiplets and hyper multiplets around\\r\\n$Q$-invariant off-shell configurations in four-dimensional $\\\\mathcal{N}=2$\\r\\nsupergravity with $AdS_{2} \\\\times S^{2}$ boundary conditions, using the\\r\\nAtiyah-Bott fixed-point index theorem and a subsequent zeta function\\r\\nregularization. Our results extend the large-charge on-shell entropy\\r\\ncomputations in the literature to a regime of finite charges. Based on our\\r\\nresults, we present an exact formula for the quantum entropy of BPS black holes\\r\\nin $\\\\mathcal{N}=2$ supergravity. We explain cancellations concerning\\r\\n$\\\\frac18$-BPS black holes in $\\\\mathcal{N}=8$ supergravity that were observed in\\r\\narXiv:1111.1161. We also make comments about the interpretation of a\\r\\nlogarithmic term in the topological string partition function in the low energy\\r\\nsupergravity theory.\\r\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bare_sample_df.text[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's map those into a single 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask all LaTeX with a single word ' _LATEX_ '\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DeLaTeX(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Replace '\\$(.+)?\\$' with ' _LATEX_ '\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.str.replace(r'\\$(.+)?\\$', ' _LATEX_ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Functional determinants, index theorems, and exact quantum black hole\\r\\n  entropy   The exact quantum entropy of BPS black holes can be evaluated using\\r\\nlocalization in supergravity. An important ingredient in this program, that has\\r\\nbeen lacking so far, is the one-loop effect arising from the quadratic\\r\\nfluctuations of the exact deformation (the  _LATEX_  operator). We compute\\r\\nthe fluctuation determinant for vector multiplets and hyper multiplets around\\r\\n _LATEX_ \\r\\nsupergravity with  _LATEX_  boundary conditions, using the\\r\\nAtiyah-Bott fixed-point index theorem and a subsequent zeta function\\r\\nregularization. Our results extend the large-charge on-shell entropy\\r\\ncomputations in the literature to a regime of finite charges. Based on our\\r\\nresults, we present an exact formula for the quantum entropy of BPS black holes\\r\\nin  _LATEX_  supergravity. We explain cancellations concerning\\r\\n _LATEX_  supergravity that were observed in\\r\\narXiv:1111.1161. We also make comments about the interpretation of a\\r\\nlogarithmic term in the topological string partition function in the low energy\\r\\nsupergravity theory.\\r\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delatex = DeLaTeX()\n",
    "delatex.transform(bare_sample_df.text)[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a simple untuned pipeline with a couple shallow classifiers\n",
    "### We relay on the *class_weight* argument to account for the imbalance of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text       object\n",
       "label    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the data_frame\n",
    "\n",
    "work_df = bare_sample_df\n",
    "\n",
    "print(len(work_df))\n",
    "work_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(work_df.text, work_df.label, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'math', 'physics', 'q-bio', 'q-fin', 'stat']\n"
     ]
    }
   ],
   "source": [
    "# encode the labels, 'cs' -> 0, ..., 'stat' -> 5\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_e = preprocessing.LabelEncoder()\n",
    "y_train = label_e.fit_transform(label_train)\n",
    "y_test = label_e.transform(label_test)\n",
    "\n",
    "print(list(label_e.classes_))\n",
    "\n",
    "#label_e.inverse_transform([0]) # array(['cs'], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first build the pipe and push the data trough step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "notex_text_train = delatex.fit_transform(text_train)\n",
    "notex_text_test = delatex.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10805, 19250)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_v = CountVectorizer(strip_accents='unicode', min_df = 2, max_df = 0.8)\n",
    "word_counts_train = count_v.fit_transform(notex_text_train)\n",
    "word_counts_test = count_v.transform(notex_text_test)\n",
    "word_counts_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is there fewer rows? Were they empty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21230, 'is'),\n",
       " (20436, 'for'),\n",
       " (15495, 'that'),\n",
       " (14938, '_latex_'),\n",
       " (14689, 'with'),\n",
       " (12535, 'on'),\n",
       " (11021, 'this'),\n",
       " (10733, 'are'),\n",
       " (10459, 'by'),\n",
       " (8571, 'as'),\n",
       " (8240, 'an'),\n",
       " (6780, 'be'),\n",
       " (6578, 'from'),\n",
       " (6384, 'which'),\n",
       " (5596, 'at'),\n",
       " (5067, 'can'),\n",
       " (4967, 'model'),\n",
       " (4231, 'two'),\n",
       " (4055, 'our'),\n",
       " (4035, 'it'),\n",
       " (3663, 'these'),\n",
       " (3630, 'results'),\n",
       " (3625, 'using'),\n",
       " (3616, 'show'),\n",
       " (3377, 'time'),\n",
       " (3367, 'data'),\n",
       " (3305, 'also'),\n",
       " (3185, 'based'),\n",
       " (3029, 'between'),\n",
       " (3006, 'or'),\n",
       " (2914, 'field'),\n",
       " (2886, 'paper'),\n",
       " (2851, 'quantum'),\n",
       " (2778, 'one'),\n",
       " (2763, 'have')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the most frequent words (after using the 'max_df' above)\n",
    "\n",
    "sum_word_counts_train = word_counts_train.sum(axis=0)\n",
    "sorted([(sum_word_counts_train[0, i], word) for word, i in count_v.vocabulary_.items()],reverse=True)[:35]\n",
    "\n",
    "# There's a lot of '_latex_'.\n",
    "# Yet I would still hope that it is underrepresented in quantitative biology or quantitative finance\n",
    "#\n",
    "# Maybe we should have balanced the classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def build_arXivMeta_clf(model):\n",
    "    pipe = []\n",
    "    pipe.append(( 'delatex', DeLaTeX() ))\n",
    "    pipe.append(( 'count_v', CountVectorizer(strip_accents='unicode', min_df = 2, max_df = 0.8)  ))\n",
    "    pipe.append(( 'tfidf_t', TfidfTransformer(use_idf=False)  ))\n",
    "    pipe.append(( 'sgd_clf', model  ))\n",
    "\n",
    "    return Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('delatex', DeLaTeX()), ('count_v', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, sto..._state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our first model, an linear SVMM with stochastic gradient descent\n",
    "\n",
    "arXivMeta_clf_1 = build_arXivMeta_clf(\n",
    "    SGDClassifier(loss='hinge', class_weight=\"balanced\", n_jobs=-1, max_iter=1000, tol=1e-3)\n",
    ")\n",
    "arXivMeta_clf_1.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, let's just have some fun first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cs', 'physics', 'q-bio', 'cs', 'math', 'q-fin', 'stat'], dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_abstracts = pd.Series([\"\"\"\n",
    "The Lack of A Priori Distinctions Between Learning Algorithms  This is the first of\n",
    "two papers that use off-training set (OTS) error to investigate the assumption-free\n",
    "relationship between learning algorithms. This first paper discusses the senses in\n",
    "which there are no a priori distinctions between learning algorithms. (The second\n",
    "paper discusses the senses in which there are such distinctions.) In this first paper\n",
    "it is shown, loosely speaking, that for any two algorithms A and B, there are \"as many\"\n",
    "targets (or priors over targets) for which A has lower expected OTS error than B as\n",
    "vice versa, for loss functions like zero-one loss. In particular, this is true if A\n",
    "is cross-validation and B is \"anti-cross-validation'' (choose the learning algorithm\n",
    "with largest cross-validation error). This paper ends with a discussion of the\n",
    "implications of these results for computational learning theory. It is shown that one\n",
    "cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis\n",
    "dimension of your generalizer is small, and the training set is large, then with high\n",
    "probability your OTS error is small. Other implications for \"membership queries\"\n",
    "algorithms and \"punting\" algorithms are also discussed.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "X-rays quarks lepton scattering experiment field\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "genes DNA RNA sequencing protein species fenotype \n",
    "\"\"\",\n",
    "\"\"\"\n",
    "computer algorithm graph sorting depth first interface\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "We offer a novel less intuitive proof of $\\limit_{x\\to 0} x = 0$,\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "infllation resources market stock bonds derivatives\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "distribution Bayesian p value marginalization Monte Carlo\n",
    "\"\"\"\n",
    "])\n",
    "\n",
    "\n",
    "label_e.inverse_transform(arXivMeta_clf_1.predict(random_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look promising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the multilabel classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def show_metrics(clf):\n",
    "    predicted_y_test = clf.predict(text_test)\n",
    "    print(metrics.classification_report(y_test, predicted_y_test, target_names=label_e.classes_))\n",
    "    print(metrics.confusion_matrix(y_test, predicted_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          cs       0.68      0.73      0.70       894\n",
      "        math       0.80      0.73      0.76      1510\n",
      "     physics       0.93      0.90      0.92      2633\n",
      "       q-bio       0.44      0.59      0.51        80\n",
      "       q-fin       0.58      0.50      0.54        30\n",
      "        stat       0.34      0.59      0.43       176\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      5323\n",
      "   macro avg       0.63      0.67      0.64      5323\n",
      "weighted avg       0.82      0.81      0.81      5323\n",
      "\n",
      "[[ 652  104   31   11    3   93]\n",
      " [ 195 1106  122   10    8   69]\n",
      " [  71  140 2364   28    0   30]\n",
      " [  12    3   10   47    0    8]\n",
      " [   2    7    1    0   15    5]\n",
      " [  31   28    3   10    0  104]]\n"
     ]
    }
   ],
   "source": [
    "show_metrics(arXivMeta_clf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: understand what the above values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('delatex', DeLaTeX()), ('count_v', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, sto...tors=100, n_jobs=None, oob_score=False,\n",
       "            random_state=123, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "arXivMeta_clf_2 = build_arXivMeta_clf(\n",
    "    RandomForestClassifier(class_weight = 'balanced', n_estimators=100, max_depth=10, criterion='entropy', random_state=123)\n",
    ")\n",
    "arXivMeta_clf_2.fit(text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          cs       0.61      0.59      0.60       894\n",
      "        math       0.67      0.74      0.70      1510\n",
      "     physics       0.89      0.81      0.85      2633\n",
      "       q-bio       0.31      0.41      0.35        80\n",
      "       q-fin       0.62      0.43      0.51        30\n",
      "        stat       0.33      0.48      0.40       176\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      5323\n",
      "   macro avg       0.57      0.58      0.57      5323\n",
      "weighted avg       0.75      0.74      0.74      5323\n",
      "\n",
      "[[ 530  198   64   15    3   84]\n",
      " [ 176 1119  156    7    5   47]\n",
      " [  96  318 2145   45    0   29]\n",
      " [  15    3   22   33    0    7]\n",
      " [   4    9    2    0   13    2]\n",
      " [  46   30    8    7    0   85]]\n"
     ]
    }
   ],
   "source": [
    "show_metrics(arXivMeta_clf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# make the confusion matrix with a heat_map  \n",
    "\n",
    "# grid search with cross validation\n",
    "# try Hash Vectorizer instead of CountVectorizer\n",
    "# balance the classes by up-/down-sampling and use LogisticRegression, NaiveBayes, ...\n",
    "# out-of-core learning ?\n",
    "\n",
    "# see how accuracy scales with data volume (we have 800 MB to go around)\n",
    "\n",
    "# unsupervised learning:\n",
    "# LDA\n",
    "# clustering\n",
    "# visualization: t-SNE ?\n",
    "\n",
    "# deep learning ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
